# Defenses Against Attacks by Content
List of papers on cognitive security for AI agents.

[![Maintenance](https://img.shields.io/badge/Maintained%3F-yes-green.svg)]([https://github.com/Cartus/Automated-Fact-Checking-Literature](https://github.com/MichSchli/AgentCogSec))
[![Last Commit](https://img.shields.io/github/last-commit/MichSchli/AgentCogSec)](https://github.com/MichSchli/AgentCogSec)
[![Contribution_welcome](https://img.shields.io/badge/Contributions-welcome-blue)](https://github.com/MichSchli/AgentCogSec/blob/main/contribute.md)

## Overview
This repo contains relevant resources from []. In this paper, we introduce attacks by content, a type of prompt injection where an attacker manipulates a RAG system or an AI agent by supplying biased, misleading, or false information. This differs from traditional prompt injection in that the surface form of the message is indistinguishable from legitimate content. The agent must therefore analyse the *content* of the message to identify the attack -- i.e., the agent must fact-check. In this repository, we curate a list of papers focusing on defending against such attacks. As the field evolves, we will provide timely updates in this repository.

- [Task Definition](#task-definition)
- [Claim Prioritisation](#claim-prioritisation)


## Task Definition
Figure below ...

## Claim Prioritisation

Strategies for integrating untrustworthy retrieved documents without relying on external evidence. We propose that such methods can fruitfully be combined with evidence-based reasoning, serving as a first line of defense and limiting expensive searches for additional evidence.

* [[Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models]](https://arxiv.org/abs/2410.07176).
* [[To Trust or Not to Trust? Enhancing Large Language Models' Situated Faithfulness to External Contexts]](https://openreview.net/forum?id=K2jOacHUlO).
* 


## Evidence Retrieval

Bla

## Source Criticism & Veracity Analysis

Bla
